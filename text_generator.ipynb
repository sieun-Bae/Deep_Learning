{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sieun-Bae/deep-learning/blob/master/text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG6rM9KwY5g9",
        "colab_type": "text"
      },
      "source": [
        "### 1. Use hard-coded text to provide training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh5jntXwZDzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3fe4c7b-45d3-408b-c081-27df19a422d6"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = \"The series begins with the return of Upper East Side teenage it girl \\\n",
        "        Serena van der Woodsen (Blake Lively) from a mysterious absence. \\\n",
        "        She is reunited with her frenemy Blair Waldorf (Leighton Meester) and \\\n",
        "        her mother Lily van der Woodsen (Kelly Rutherford), while she also meets \\\n",
        "        Dan Humphrey (Penn Badgley), an aspiring writer from Brooklyn. \\\n",
        "        Other main characters are played by Chace Crawford, Ed Westwick, Taylor \\\n",
        "        Momsen, Jessica Szohr, Matthew Settle and Kaylee DeFer.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding='pre'))\n",
        "\n",
        "xs = input_sequences[:, :-1]\n",
        "labels = input_sequences[:, -1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length = max_sequence_len -1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(xs, ys, epochs = 500, verbose = 1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 73 samples\n",
            "Epoch 1/500\n",
            "73/73 [==============================] - 1s 16ms/sample - loss: 4.1913 - acc: 0.0274\n",
            "Epoch 2/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1858 - acc: 0.0411\n",
            "Epoch 3/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 4.1815 - acc: 0.0411\n",
            "Epoch 4/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.1774 - acc: 0.0685\n",
            "Epoch 5/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1737 - acc: 0.0685\n",
            "Epoch 6/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1694 - acc: 0.0685\n",
            "Epoch 7/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1651 - acc: 0.0959\n",
            "Epoch 8/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1602 - acc: 0.1096\n",
            "Epoch 9/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 4.1551 - acc: 0.1096\n",
            "Epoch 10/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.1495 - acc: 0.1096\n",
            "Epoch 11/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.1430 - acc: 0.1096\n",
            "Epoch 12/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.1358 - acc: 0.1096\n",
            "Epoch 13/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1277 - acc: 0.1096\n",
            "Epoch 14/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.1186 - acc: 0.1233\n",
            "Epoch 15/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.1073 - acc: 0.1233\n",
            "Epoch 16/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.0937 - acc: 0.1370\n",
            "Epoch 17/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.0782 - acc: 0.1370\n",
            "Epoch 18/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.0577 - acc: 0.1233\n",
            "Epoch 19/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 4.0327 - acc: 0.1370\n",
            "Epoch 20/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 4.0078 - acc: 0.1233\n",
            "Epoch 21/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.9802 - acc: 0.1233\n",
            "Epoch 22/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.9505 - acc: 0.1096\n",
            "Epoch 23/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.9198 - acc: 0.1096\n",
            "Epoch 24/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.8811 - acc: 0.1096\n",
            "Epoch 25/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.8480 - acc: 0.1096\n",
            "Epoch 26/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.8129 - acc: 0.1370\n",
            "Epoch 27/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.7813 - acc: 0.1233\n",
            "Epoch 28/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.7451 - acc: 0.1370\n",
            "Epoch 29/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.7088 - acc: 0.1370\n",
            "Epoch 30/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.6712 - acc: 0.1781\n",
            "Epoch 31/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.6418 - acc: 0.1781\n",
            "Epoch 32/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 3.6052 - acc: 0.1918\n",
            "Epoch 33/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.5667 - acc: 0.1918\n",
            "Epoch 34/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.5348 - acc: 0.1918\n",
            "Epoch 35/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.5024 - acc: 0.1507\n",
            "Epoch 36/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.4702 - acc: 0.1781\n",
            "Epoch 37/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.4404 - acc: 0.1918\n",
            "Epoch 38/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.4096 - acc: 0.1918\n",
            "Epoch 39/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.3745 - acc: 0.2055\n",
            "Epoch 40/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.3444 - acc: 0.1781\n",
            "Epoch 41/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.3135 - acc: 0.1918\n",
            "Epoch 42/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.2810 - acc: 0.1918\n",
            "Epoch 43/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.2509 - acc: 0.1918\n",
            "Epoch 44/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.2184 - acc: 0.2055\n",
            "Epoch 45/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.1861 - acc: 0.2192\n",
            "Epoch 46/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.1611 - acc: 0.2055\n",
            "Epoch 47/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 3.1327 - acc: 0.2192\n",
            "Epoch 48/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 3.1020 - acc: 0.2466\n",
            "Epoch 49/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.0765 - acc: 0.2329\n",
            "Epoch 50/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.0525 - acc: 0.2466\n",
            "Epoch 51/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 3.0251 - acc: 0.2466\n",
            "Epoch 52/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.9960 - acc: 0.2466\n",
            "Epoch 53/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.9725 - acc: 0.2603\n",
            "Epoch 54/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.9506 - acc: 0.2466\n",
            "Epoch 55/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.9258 - acc: 0.2603\n",
            "Epoch 56/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.8973 - acc: 0.2466\n",
            "Epoch 57/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.8766 - acc: 0.2466\n",
            "Epoch 58/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.8559 - acc: 0.2192\n",
            "Epoch 59/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.8305 - acc: 0.1918\n",
            "Epoch 60/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.8057 - acc: 0.2329\n",
            "Epoch 61/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.7786 - acc: 0.2466\n",
            "Epoch 62/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.7550 - acc: 0.2466\n",
            "Epoch 63/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.7333 - acc: 0.2603\n",
            "Epoch 64/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.7096 - acc: 0.2740\n",
            "Epoch 65/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.6867 - acc: 0.2877\n",
            "Epoch 66/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.6656 - acc: 0.2877\n",
            "Epoch 67/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.6448 - acc: 0.2877\n",
            "Epoch 68/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.6222 - acc: 0.3014\n",
            "Epoch 69/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.6001 - acc: 0.3014\n",
            "Epoch 70/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.5781 - acc: 0.3288\n",
            "Epoch 71/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.5555 - acc: 0.3151\n",
            "Epoch 72/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.5346 - acc: 0.3014\n",
            "Epoch 73/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.5173 - acc: 0.2740\n",
            "Epoch 74/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.5043 - acc: 0.3014\n",
            "Epoch 75/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.4741 - acc: 0.2877\n",
            "Epoch 76/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.4544 - acc: 0.3151\n",
            "Epoch 77/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.4281 - acc: 0.3425\n",
            "Epoch 78/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.4126 - acc: 0.3425\n",
            "Epoch 79/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.3906 - acc: 0.3699\n",
            "Epoch 80/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.3726 - acc: 0.3973\n",
            "Epoch 81/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.3519 - acc: 0.4110\n",
            "Epoch 82/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.3316 - acc: 0.4247\n",
            "Epoch 83/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.3160 - acc: 0.4384\n",
            "Epoch 84/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.2974 - acc: 0.4384\n",
            "Epoch 85/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.2782 - acc: 0.4658\n",
            "Epoch 86/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.2715 - acc: 0.5068\n",
            "Epoch 87/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.2564 - acc: 0.4932\n",
            "Epoch 88/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.2235 - acc: 0.4658\n",
            "Epoch 89/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.2131 - acc: 0.4932\n",
            "Epoch 90/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1950 - acc: 0.5068\n",
            "Epoch 91/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1773 - acc: 0.5342\n",
            "Epoch 92/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1636 - acc: 0.5205\n",
            "Epoch 93/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1491 - acc: 0.4658\n",
            "Epoch 94/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.1324 - acc: 0.4795\n",
            "Epoch 95/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1227 - acc: 0.4658\n",
            "Epoch 96/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.1040 - acc: 0.4795\n",
            "Epoch 97/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.0944 - acc: 0.5205\n",
            "Epoch 98/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.0722 - acc: 0.5479\n",
            "Epoch 99/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.0714 - acc: 0.5753\n",
            "Epoch 100/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 2.0472 - acc: 0.5890\n",
            "Epoch 101/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.0403 - acc: 0.6027\n",
            "Epoch 102/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 2.0235 - acc: 0.6027\n",
            "Epoch 103/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 2.0069 - acc: 0.6027\n",
            "Epoch 104/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.9962 - acc: 0.5616\n",
            "Epoch 105/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.9813 - acc: 0.5890\n",
            "Epoch 106/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.9654 - acc: 0.6027\n",
            "Epoch 107/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.9548 - acc: 0.6164\n",
            "Epoch 108/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.9406 - acc: 0.6027\n",
            "Epoch 109/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.9288 - acc: 0.5890\n",
            "Epoch 110/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.9184 - acc: 0.5753\n",
            "Epoch 111/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.9136 - acc: 0.5890\n",
            "Epoch 112/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.8934 - acc: 0.6164\n",
            "Epoch 113/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.8881 - acc: 0.6027\n",
            "Epoch 114/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8732 - acc: 0.6438\n",
            "Epoch 115/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.8648 - acc: 0.5890\n",
            "Epoch 116/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8677 - acc: 0.6027\n",
            "Epoch 117/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8462 - acc: 0.6301\n",
            "Epoch 118/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8358 - acc: 0.6301\n",
            "Epoch 119/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8236 - acc: 0.6438\n",
            "Epoch 120/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8039 - acc: 0.6986\n",
            "Epoch 121/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.8176 - acc: 0.6712\n",
            "Epoch 122/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7835 - acc: 0.6575\n",
            "Epoch 123/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.7937 - acc: 0.6438\n",
            "Epoch 124/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7675 - acc: 0.6712\n",
            "Epoch 125/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7664 - acc: 0.6301\n",
            "Epoch 126/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7473 - acc: 0.6712\n",
            "Epoch 127/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.7440 - acc: 0.6849\n",
            "Epoch 128/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.7331 - acc: 0.7260\n",
            "Epoch 129/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.7180 - acc: 0.7397\n",
            "Epoch 130/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7435 - acc: 0.6712\n",
            "Epoch 131/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6957 - acc: 0.7534\n",
            "Epoch 132/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.6915 - acc: 0.7260\n",
            "Epoch 133/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.7004 - acc: 0.6849\n",
            "Epoch 134/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6705 - acc: 0.7945\n",
            "Epoch 135/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.6644 - acc: 0.7260\n",
            "Epoch 136/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6452 - acc: 0.7671\n",
            "Epoch 137/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.6463 - acc: 0.7671\n",
            "Epoch 138/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6350 - acc: 0.7397\n",
            "Epoch 139/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.6183 - acc: 0.7945\n",
            "Epoch 140/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6187 - acc: 0.7534\n",
            "Epoch 141/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.6026 - acc: 0.7534\n",
            "Epoch 142/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5943 - acc: 0.7808\n",
            "Epoch 143/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5867 - acc: 0.7945\n",
            "Epoch 144/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5760 - acc: 0.8356\n",
            "Epoch 145/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5688 - acc: 0.7945\n",
            "Epoch 146/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.5569 - acc: 0.8219\n",
            "Epoch 147/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5506 - acc: 0.8356\n",
            "Epoch 148/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5395 - acc: 0.8356\n",
            "Epoch 149/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.5312 - acc: 0.8493\n",
            "Epoch 150/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.5215 - acc: 0.8767\n",
            "Epoch 151/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5132 - acc: 0.8493\n",
            "Epoch 152/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.5055 - acc: 0.8493\n",
            "Epoch 153/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4999 - acc: 0.8082\n",
            "Epoch 154/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4930 - acc: 0.8356\n",
            "Epoch 155/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.4858 - acc: 0.8082\n",
            "Epoch 156/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4783 - acc: 0.8219\n",
            "Epoch 157/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.4666 - acc: 0.8219\n",
            "Epoch 158/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4556 - acc: 0.8219\n",
            "Epoch 159/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.4547 - acc: 0.8356\n",
            "Epoch 160/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4434 - acc: 0.8493\n",
            "Epoch 161/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4391 - acc: 0.8630\n",
            "Epoch 162/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.4264 - acc: 0.8630\n",
            "Epoch 163/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4233 - acc: 0.8904\n",
            "Epoch 164/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.4123 - acc: 0.8767\n",
            "Epoch 165/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.4079 - acc: 0.9041\n",
            "Epoch 166/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3991 - acc: 0.8767\n",
            "Epoch 167/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3917 - acc: 0.8767\n",
            "Epoch 168/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3828 - acc: 0.8630\n",
            "Epoch 169/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3828 - acc: 0.8493\n",
            "Epoch 170/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3720 - acc: 0.8904\n",
            "Epoch 171/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.3644 - acc: 0.8767\n",
            "Epoch 172/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3584 - acc: 0.8493\n",
            "Epoch 173/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3478 - acc: 0.8767\n",
            "Epoch 174/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.3422 - acc: 0.8904\n",
            "Epoch 175/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3377 - acc: 0.9178\n",
            "Epoch 176/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3314 - acc: 0.8767\n",
            "Epoch 177/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3238 - acc: 0.8630\n",
            "Epoch 178/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3200 - acc: 0.8904\n",
            "Epoch 179/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.3093 - acc: 0.8904\n",
            "Epoch 180/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.3021 - acc: 0.8904\n",
            "Epoch 181/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2968 - acc: 0.9178\n",
            "Epoch 182/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2914 - acc: 0.9452\n",
            "Epoch 183/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2830 - acc: 0.9315\n",
            "Epoch 184/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2771 - acc: 0.9315\n",
            "Epoch 185/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2721 - acc: 0.9315\n",
            "Epoch 186/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2678 - acc: 0.9041\n",
            "Epoch 187/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2603 - acc: 0.9041\n",
            "Epoch 188/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2545 - acc: 0.9041\n",
            "Epoch 189/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2475 - acc: 0.9041\n",
            "Epoch 190/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2420 - acc: 0.9178\n",
            "Epoch 191/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2373 - acc: 0.8904\n",
            "Epoch 192/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2309 - acc: 0.8904\n",
            "Epoch 193/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.2238 - acc: 0.8767\n",
            "Epoch 194/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2230 - acc: 0.9315\n",
            "Epoch 195/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2144 - acc: 0.9315\n",
            "Epoch 196/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.2106 - acc: 0.9178\n",
            "Epoch 197/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.2019 - acc: 0.9315\n",
            "Epoch 198/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.1961 - acc: 0.9315\n",
            "Epoch 199/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1900 - acc: 0.9178\n",
            "Epoch 200/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1876 - acc: 0.9041\n",
            "Epoch 201/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1793 - acc: 0.8904\n",
            "Epoch 202/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.1752 - acc: 0.8904\n",
            "Epoch 203/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1686 - acc: 0.8904\n",
            "Epoch 204/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1653 - acc: 0.8904\n",
            "Epoch 205/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1600 - acc: 0.8904\n",
            "Epoch 206/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1528 - acc: 0.8904\n",
            "Epoch 207/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.1482 - acc: 0.9178\n",
            "Epoch 208/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1426 - acc: 0.9178\n",
            "Epoch 209/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.1366 - acc: 0.9315\n",
            "Epoch 210/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1309 - acc: 0.9178\n",
            "Epoch 211/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.1253 - acc: 0.9315\n",
            "Epoch 212/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.1208 - acc: 0.9315\n",
            "Epoch 213/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1156 - acc: 0.9315\n",
            "Epoch 214/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1101 - acc: 0.9315\n",
            "Epoch 215/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1061 - acc: 0.9452\n",
            "Epoch 216/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.1006 - acc: 0.9315\n",
            "Epoch 217/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0946 - acc: 0.9452\n",
            "Epoch 218/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0880 - acc: 0.9315\n",
            "Epoch 219/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0841 - acc: 0.9315\n",
            "Epoch 220/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0790 - acc: 0.9452\n",
            "Epoch 221/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0730 - acc: 0.9452\n",
            "Epoch 222/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.0690 - acc: 0.9452\n",
            "Epoch 223/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0660 - acc: 0.9452\n",
            "Epoch 224/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.0607 - acc: 0.9589\n",
            "Epoch 225/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.0562 - acc: 0.9452\n",
            "Epoch 226/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0508 - acc: 0.9589\n",
            "Epoch 227/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0452 - acc: 0.9726\n",
            "Epoch 228/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.0413 - acc: 0.9589\n",
            "Epoch 229/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.0374 - acc: 0.9726\n",
            "Epoch 230/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.0363 - acc: 0.9726\n",
            "Epoch 231/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0296 - acc: 0.9726\n",
            "Epoch 232/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 1.0307 - acc: 0.9452\n",
            "Epoch 233/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.0179 - acc: 0.9589\n",
            "Epoch 234/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0091 - acc: 0.9726\n",
            "Epoch 235/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 1.0058 - acc: 0.9726\n",
            "Epoch 236/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 1.0023 - acc: 0.9726\n",
            "Epoch 237/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.9965 - acc: 0.9726\n",
            "Epoch 238/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9914 - acc: 0.9726\n",
            "Epoch 239/500\n",
            "73/73 [==============================] - 1s 9ms/sample - loss: 0.9875 - acc: 0.9589\n",
            "Epoch 240/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.9809 - acc: 0.9726\n",
            "Epoch 241/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9763 - acc: 0.9726\n",
            "Epoch 242/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9703 - acc: 0.9726\n",
            "Epoch 243/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9657 - acc: 0.9726\n",
            "Epoch 244/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.9600 - acc: 0.9726\n",
            "Epoch 245/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9547 - acc: 0.9726\n",
            "Epoch 246/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9480 - acc: 0.9726\n",
            "Epoch 247/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9465 - acc: 0.9726\n",
            "Epoch 248/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9390 - acc: 0.9726\n",
            "Epoch 249/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.9355 - acc: 0.9726\n",
            "Epoch 250/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9299 - acc: 0.9726\n",
            "Epoch 251/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9268 - acc: 0.9726\n",
            "Epoch 252/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9228 - acc: 0.9726\n",
            "Epoch 253/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9207 - acc: 0.9726\n",
            "Epoch 254/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9156 - acc: 0.9726\n",
            "Epoch 255/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.9130 - acc: 0.9726\n",
            "Epoch 256/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9113 - acc: 0.9589\n",
            "Epoch 257/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9037 - acc: 0.9589\n",
            "Epoch 258/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9121 - acc: 0.9726\n",
            "Epoch 259/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8945 - acc: 0.9726\n",
            "Epoch 260/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9018 - acc: 0.9726\n",
            "Epoch 261/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8880 - acc: 0.9726\n",
            "Epoch 262/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8970 - acc: 0.9726\n",
            "Epoch 263/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8943 - acc: 0.9589\n",
            "Epoch 264/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9026 - acc: 0.9589\n",
            "Epoch 265/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.9030 - acc: 0.9589\n",
            "Epoch 266/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.9099 - acc: 0.9726\n",
            "Epoch 267/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8969 - acc: 0.9726\n",
            "Epoch 268/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8956 - acc: 0.9726\n",
            "Epoch 269/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.8899 - acc: 0.9589\n",
            "Epoch 270/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8911 - acc: 0.9589\n",
            "Epoch 271/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8811 - acc: 0.9589\n",
            "Epoch 272/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8737 - acc: 0.9863\n",
            "Epoch 273/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8725 - acc: 0.9726\n",
            "Epoch 274/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8645 - acc: 0.9863\n",
            "Epoch 275/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8620 - acc: 0.9863\n",
            "Epoch 276/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.8559 - acc: 0.9863\n",
            "Epoch 277/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8508 - acc: 0.9863\n",
            "Epoch 278/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8440 - acc: 0.9863\n",
            "Epoch 279/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8380 - acc: 0.9863\n",
            "Epoch 280/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8327 - acc: 0.9863\n",
            "Epoch 281/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8267 - acc: 0.9863\n",
            "Epoch 282/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8210 - acc: 0.9863\n",
            "Epoch 283/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.8203 - acc: 0.9726\n",
            "Epoch 284/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8140 - acc: 0.9863\n",
            "Epoch 285/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.8105 - acc: 0.9863\n",
            "Epoch 286/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8061 - acc: 0.9863\n",
            "Epoch 287/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8032 - acc: 0.9863\n",
            "Epoch 288/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7997 - acc: 1.0000\n",
            "Epoch 289/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7973 - acc: 1.0000\n",
            "Epoch 290/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7986 - acc: 1.0000\n",
            "Epoch 291/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7951 - acc: 0.9863\n",
            "Epoch 292/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.8023 - acc: 1.0000\n",
            "Epoch 293/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7933 - acc: 0.9863\n",
            "Epoch 294/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7906 - acc: 0.9863\n",
            "Epoch 295/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7779 - acc: 0.9863\n",
            "Epoch 296/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7811 - acc: 0.9726\n",
            "Epoch 297/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7686 - acc: 0.9863\n",
            "Epoch 298/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7678 - acc: 1.0000\n",
            "Epoch 299/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7605 - acc: 1.0000\n",
            "Epoch 300/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7568 - acc: 1.0000\n",
            "Epoch 301/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7542 - acc: 1.0000\n",
            "Epoch 302/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7556 - acc: 1.0000\n",
            "Epoch 303/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.7482 - acc: 1.0000\n",
            "Epoch 304/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7433 - acc: 1.0000\n",
            "Epoch 305/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.7431 - acc: 1.0000\n",
            "Epoch 306/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7488 - acc: 0.9863\n",
            "Epoch 307/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7479 - acc: 0.9863\n",
            "Epoch 308/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7376 - acc: 1.0000\n",
            "Epoch 309/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7367 - acc: 1.0000\n",
            "Epoch 310/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7234 - acc: 1.0000\n",
            "Epoch 311/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7279 - acc: 1.0000\n",
            "Epoch 312/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7216 - acc: 1.0000\n",
            "Epoch 313/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7166 - acc: 1.0000\n",
            "Epoch 314/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7207 - acc: 1.0000\n",
            "Epoch 315/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.7203 - acc: 0.9863\n",
            "Epoch 316/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.7100 - acc: 1.0000\n",
            "Epoch 317/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7031 - acc: 1.0000\n",
            "Epoch 318/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6980 - acc: 1.0000\n",
            "Epoch 319/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6945 - acc: 1.0000\n",
            "Epoch 320/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6916 - acc: 1.0000\n",
            "Epoch 321/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6907 - acc: 1.0000\n",
            "Epoch 322/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6869 - acc: 1.0000\n",
            "Epoch 323/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6833 - acc: 1.0000\n",
            "Epoch 324/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6862 - acc: 0.9863\n",
            "Epoch 325/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6783 - acc: 1.0000\n",
            "Epoch 326/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6862 - acc: 1.0000\n",
            "Epoch 327/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6786 - acc: 1.0000\n",
            "Epoch 328/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6867 - acc: 1.0000\n",
            "Epoch 329/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.7029 - acc: 0.9863\n",
            "Epoch 330/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6707 - acc: 1.0000\n",
            "Epoch 331/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6842 - acc: 1.0000\n",
            "Epoch 332/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6658 - acc: 1.0000\n",
            "Epoch 333/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6726 - acc: 1.0000\n",
            "Epoch 334/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6581 - acc: 1.0000\n",
            "Epoch 335/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6592 - acc: 0.9863\n",
            "Epoch 336/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6491 - acc: 1.0000\n",
            "Epoch 337/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6542 - acc: 1.0000\n",
            "Epoch 338/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6432 - acc: 1.0000\n",
            "Epoch 339/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6531 - acc: 1.0000\n",
            "Epoch 340/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6455 - acc: 0.9863\n",
            "Epoch 341/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6576 - acc: 0.9863\n",
            "Epoch 342/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6349 - acc: 1.0000\n",
            "Epoch 343/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6447 - acc: 0.9863\n",
            "Epoch 344/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6277 - acc: 1.0000\n",
            "Epoch 345/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6480 - acc: 0.9726\n",
            "Epoch 346/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6434 - acc: 1.0000\n",
            "Epoch 347/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6255 - acc: 1.0000\n",
            "Epoch 348/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6397 - acc: 1.0000\n",
            "Epoch 349/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6284 - acc: 1.0000\n",
            "Epoch 350/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6123 - acc: 1.0000\n",
            "Epoch 351/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6205 - acc: 1.0000\n",
            "Epoch 352/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6034 - acc: 1.0000\n",
            "Epoch 353/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6093 - acc: 1.0000\n",
            "Epoch 354/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6035 - acc: 1.0000\n",
            "Epoch 355/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6117 - acc: 1.0000\n",
            "Epoch 356/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.6049 - acc: 1.0000\n",
            "Epoch 357/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.6037 - acc: 1.0000\n",
            "Epoch 358/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.6000 - acc: 1.0000\n",
            "Epoch 359/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5868 - acc: 1.0000\n",
            "Epoch 360/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5857 - acc: 1.0000\n",
            "Epoch 361/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5854 - acc: 1.0000\n",
            "Epoch 362/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5841 - acc: 1.0000\n",
            "Epoch 363/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5768 - acc: 1.0000\n",
            "Epoch 364/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5789 - acc: 1.0000\n",
            "Epoch 365/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5775 - acc: 1.0000\n",
            "Epoch 366/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5752 - acc: 1.0000\n",
            "Epoch 367/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5727 - acc: 1.0000\n",
            "Epoch 368/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5672 - acc: 1.0000\n",
            "Epoch 369/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5652 - acc: 1.0000\n",
            "Epoch 370/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5640 - acc: 1.0000\n",
            "Epoch 371/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5623 - acc: 1.0000\n",
            "Epoch 372/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5594 - acc: 1.0000\n",
            "Epoch 373/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5544 - acc: 1.0000\n",
            "Epoch 374/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5499 - acc: 1.0000\n",
            "Epoch 375/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5528 - acc: 1.0000\n",
            "Epoch 376/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5515 - acc: 1.0000\n",
            "Epoch 377/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5494 - acc: 1.0000\n",
            "Epoch 378/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5460 - acc: 1.0000\n",
            "Epoch 379/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5403 - acc: 1.0000\n",
            "Epoch 380/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5380 - acc: 1.0000\n",
            "Epoch 381/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5360 - acc: 1.0000\n",
            "Epoch 382/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5341 - acc: 0.9863\n",
            "Epoch 383/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5307 - acc: 0.9863\n",
            "Epoch 384/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5284 - acc: 0.9863\n",
            "Epoch 385/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5246 - acc: 0.9863\n",
            "Epoch 386/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5226 - acc: 0.9863\n",
            "Epoch 387/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5210 - acc: 0.9863\n",
            "Epoch 388/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5207 - acc: 0.9863\n",
            "Epoch 389/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5208 - acc: 0.9863\n",
            "Epoch 390/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5156 - acc: 0.9863\n",
            "Epoch 391/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.5151 - acc: 0.9863\n",
            "Epoch 392/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5108 - acc: 0.9863\n",
            "Epoch 393/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5092 - acc: 0.9863\n",
            "Epoch 394/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5088 - acc: 0.9863\n",
            "Epoch 395/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5071 - acc: 0.9863\n",
            "Epoch 396/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.5040 - acc: 0.9863\n",
            "Epoch 397/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.5017 - acc: 0.9863\n",
            "Epoch 398/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4983 - acc: 0.9863\n",
            "Epoch 399/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4956 - acc: 0.9863\n",
            "Epoch 400/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4939 - acc: 0.9863\n",
            "Epoch 401/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4942 - acc: 0.9863\n",
            "Epoch 402/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4912 - acc: 0.9863\n",
            "Epoch 403/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4896 - acc: 1.0000\n",
            "Epoch 404/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4849 - acc: 1.0000\n",
            "Epoch 405/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4859 - acc: 1.0000\n",
            "Epoch 406/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4841 - acc: 1.0000\n",
            "Epoch 407/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4810 - acc: 1.0000\n",
            "Epoch 408/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4774 - acc: 1.0000\n",
            "Epoch 409/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4761 - acc: 1.0000\n",
            "Epoch 410/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4744 - acc: 1.0000\n",
            "Epoch 411/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4745 - acc: 1.0000\n",
            "Epoch 412/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4711 - acc: 1.0000\n",
            "Epoch 413/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4739 - acc: 1.0000\n",
            "Epoch 414/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4698 - acc: 1.0000\n",
            "Epoch 415/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4665 - acc: 1.0000\n",
            "Epoch 416/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4636 - acc: 1.0000\n",
            "Epoch 417/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4626 - acc: 1.0000\n",
            "Epoch 418/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4632 - acc: 1.0000\n",
            "Epoch 419/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4600 - acc: 1.0000\n",
            "Epoch 420/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4575 - acc: 1.0000\n",
            "Epoch 421/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4534 - acc: 1.0000\n",
            "Epoch 422/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4520 - acc: 1.0000\n",
            "Epoch 423/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4497 - acc: 1.0000\n",
            "Epoch 424/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4482 - acc: 1.0000\n",
            "Epoch 425/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4463 - acc: 1.0000\n",
            "Epoch 426/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4475 - acc: 1.0000\n",
            "Epoch 427/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4421 - acc: 1.0000\n",
            "Epoch 428/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4411 - acc: 1.0000\n",
            "Epoch 429/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4391 - acc: 1.0000\n",
            "Epoch 430/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4370 - acc: 1.0000\n",
            "Epoch 431/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4357 - acc: 1.0000\n",
            "Epoch 432/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4332 - acc: 1.0000\n",
            "Epoch 433/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4315 - acc: 1.0000\n",
            "Epoch 434/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4305 - acc: 1.0000\n",
            "Epoch 435/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4284 - acc: 1.0000\n",
            "Epoch 436/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4253 - acc: 1.0000\n",
            "Epoch 437/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4250 - acc: 1.0000\n",
            "Epoch 438/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4225 - acc: 1.0000\n",
            "Epoch 439/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.4207 - acc: 1.0000\n",
            "Epoch 440/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4194 - acc: 1.0000\n",
            "Epoch 441/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4202 - acc: 1.0000\n",
            "Epoch 442/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4152 - acc: 1.0000\n",
            "Epoch 443/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4211 - acc: 1.0000\n",
            "Epoch 444/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4194 - acc: 1.0000\n",
            "Epoch 445/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4170 - acc: 1.0000\n",
            "Epoch 446/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.4125 - acc: 1.0000\n",
            "Epoch 447/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4104 - acc: 1.0000\n",
            "Epoch 448/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4057 - acc: 1.0000\n",
            "Epoch 449/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4056 - acc: 1.0000\n",
            "Epoch 450/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4038 - acc: 1.0000\n",
            "Epoch 451/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4028 - acc: 1.0000\n",
            "Epoch 452/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.4001 - acc: 1.0000\n",
            "Epoch 453/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3992 - acc: 1.0000\n",
            "Epoch 454/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3990 - acc: 1.0000\n",
            "Epoch 455/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3955 - acc: 1.0000\n",
            "Epoch 456/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3958 - acc: 1.0000\n",
            "Epoch 457/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3919 - acc: 1.0000\n",
            "Epoch 458/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.3906 - acc: 1.0000\n",
            "Epoch 459/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3889 - acc: 1.0000\n",
            "Epoch 460/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3894 - acc: 1.0000\n",
            "Epoch 461/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3859 - acc: 1.0000\n",
            "Epoch 462/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3850 - acc: 1.0000\n",
            "Epoch 463/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3824 - acc: 1.0000\n",
            "Epoch 464/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3825 - acc: 1.0000\n",
            "Epoch 465/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3804 - acc: 1.0000\n",
            "Epoch 466/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3813 - acc: 1.0000\n",
            "Epoch 467/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3779 - acc: 1.0000\n",
            "Epoch 468/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3776 - acc: 1.0000\n",
            "Epoch 469/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3751 - acc: 1.0000\n",
            "Epoch 470/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3772 - acc: 1.0000\n",
            "Epoch 471/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3746 - acc: 1.0000\n",
            "Epoch 472/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.3728 - acc: 1.0000\n",
            "Epoch 473/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3737 - acc: 1.0000\n",
            "Epoch 474/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3724 - acc: 1.0000\n",
            "Epoch 475/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3745 - acc: 1.0000\n",
            "Epoch 476/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3678 - acc: 1.0000\n",
            "Epoch 477/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.3656 - acc: 1.0000\n",
            "Epoch 478/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3639 - acc: 1.0000\n",
            "Epoch 479/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3621 - acc: 1.0000\n",
            "Epoch 480/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3599 - acc: 1.0000\n",
            "Epoch 481/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3597 - acc: 1.0000\n",
            "Epoch 482/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3570 - acc: 1.0000\n",
            "Epoch 483/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3553 - acc: 1.0000\n",
            "Epoch 484/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3536 - acc: 1.0000\n",
            "Epoch 485/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3536 - acc: 1.0000\n",
            "Epoch 486/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3528 - acc: 1.0000\n",
            "Epoch 487/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3560 - acc: 1.0000\n",
            "Epoch 488/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3491 - acc: 1.0000\n",
            "Epoch 489/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3496 - acc: 1.0000\n",
            "Epoch 490/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3465 - acc: 1.0000\n",
            "Epoch 491/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3474 - acc: 1.0000\n",
            "Epoch 492/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3443 - acc: 1.0000\n",
            "Epoch 493/500\n",
            "73/73 [==============================] - 1s 8ms/sample - loss: 0.3423 - acc: 1.0000\n",
            "Epoch 494/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3410 - acc: 1.0000\n",
            "Epoch 495/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3396 - acc: 1.0000\n",
            "Epoch 496/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3378 - acc: 1.0000\n",
            "Epoch 497/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3372 - acc: 1.0000\n",
            "Epoch 498/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.3348 - acc: 1.0000\n",
            "Epoch 499/500\n",
            "73/73 [==============================] - 1s 7ms/sample - loss: 0.3354 - acc: 1.0000\n",
            "Epoch 500/500\n",
            "73/73 [==============================] - 0s 7ms/sample - loss: 0.3331 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb574723e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8SAh8TNZKgJ",
        "colab_type": "text"
      },
      "source": [
        "### 2. Test the  text generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGL9yH5PbdSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85fb04da-eba2-47bb-c75c-be5b4f020c2e"
      },
      "source": [
        "seed_text = \"To be or not to be\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding='pre')\n",
        "  predicted = model.predict_classes(token_list, verbose=0)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To be or not to be series series begins with the return of upper east side\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "815Tc6A5bkUx",
        "colab_type": "text"
      },
      "source": [
        "### 3. Use a Larger Dataset\n",
        "\n",
        "Below is the code for Shakespeare's sonnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3962cb96-741c-47f2-bbb4-8d9eecab535d"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-27 06:37:44--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 2a00:1450:4013:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-11-27 06:37:44 (156 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUGZvV4gsYL",
        "colab_type": "text"
      },
      "source": [
        "### 4. Define neural network using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "397a1fc6-eb0c-4fde-97fc-94c7e0d15e6e"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = \"The series begins with the return of Upper East Side teenage it girl \\\n",
        "        Serena van der Woodsen (Blake Lively) from a mysterious absence. \\\n",
        "        She is reunited with her frenemy Blair Waldorf (Leighton Meester) and \\\n",
        "        her mother Lily van der Woodsen (Kelly Rutherford), while she also meets \\\n",
        "        Dan Humphrey (Penn Badgley), an aspiring writer from Brooklyn. \\\n",
        "        Other main characters are played by Chace Crawford, Ed Westwick, Taylor \\\n",
        "        Momsen, Jessica Szohr, Matthew Settle and Kaylee DeFer.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length = max_sequence_len-1)) # Try 100 dimensions\n",
        "model.add(LSTM(150)) # Try 150 units \n",
        "model.add(Dropout(0.2)) # Try dropping out 20% of neurons\n",
        "model.add(LSTM(100)) # Try 100 units\n",
        "# A Dense Layer including regularizers – not covered in lectures but useful to know\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax')) # Output layer\n",
        "adam = Adma(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
        "print(model.summary())\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7f394bf18dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Try 150 units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Try dropping out 20% of neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Try 100 units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# A Dense Layer including regularizers – not covered in lectures but useful to know\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 819\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_29 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 150]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28sHASPSh99x",
        "colab_type": "text"
      },
      "source": [
        "### 5. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIg2f1HBxqof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " history = model.fit(predictors, label, epochs=50, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0-bFPJYiL9R",
        "colab_type": "text"
      },
      "source": [
        "### 6. Plot the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fXTEO3GJ282",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjs_pizJiT4M",
        "colab_type": "text"
      },
      "source": [
        "### 7. Test the poetry generator!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}